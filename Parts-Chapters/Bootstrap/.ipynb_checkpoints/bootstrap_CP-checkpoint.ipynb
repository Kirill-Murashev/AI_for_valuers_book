{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb4316d4",
   "metadata": {},
   "source": [
    "# Development of the elements of the credit policy of the small bank with bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72775658",
   "metadata": {},
   "source": [
    "Cyrill A. Murashev, 2023-02-19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfec986",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50abd2b9",
   "metadata": {},
   "source": [
    "This material is a continuation of a cycle of work on the topic of resampling methods. We discussed the jackknife method in the previous article. Today we will look at the more popular and universal method of bootstrapping. It would not be an exaggeration to say that bootstrapping is one of the cornerstones of modern data science, along with, for example, linear regression or ROC analysis. This material was developed specifically for appraisers, so we will look at the topic from the point of view of its practical usability in the practice of collateral lending.\n",
    "\n",
    "Let's imagine a small bank operating in Eastern Europe. Its management has set a goal to develop a system of rules for the bank's credit policy. One of them should solve the task of determining the maximum price of the mortgaged property, which is safe for the financial stability of the bank. It was decided to take into account the modern reality, especially the latest achievements in the world of artificial intelligence, as well as the requirements of [IFRS 13 \"Fair Value Measurement\"](https://www.ifrs.org/issued-standards/list-of-standards/ifrs-13-fair-value-measurement/). Thus, it was decided to minimize the influence of subjective factors and maximize the influence of scientific methods. Since the capabilities of the small financial institution are rather limited, there is no opportunity to implement the full-scale AI methods based on a real big data. Instead, it is reasonable to implement some local solutions based on machine learning and apply them to the medium-sized sample.\n",
    "\n",
    "It is very important to understand the following:\n",
    "1. we are dealing with a sample data, not with the whole market data;\n",
    "1. our goal is to estimate some parameters of the market, not only for the sample.\n",
    "In this context, we should apply some special methods that can help us to bring the estimates of parameters made for the sample closer to the true values that exist on the market under consideration.\n",
    "\n",
    "Today we will explore the method of bootstrapping in its basic implementation. It allows us to improve almost any estimate made for the sample and reduce its bias relative to the true value that exists in the market.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b4b9f0",
   "metadata": {},
   "source": [
    "## Task statement and key inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478fc74",
   "metadata": {},
   "source": [
    "As mentioned above, our training task is to develop some aspects of a bank's credit policy. Imagine a local market for real estate. The bank often makes loans secured by this property. In this case, one of the tasks is to set limits for the maximum possible loan amount, as well as the maximum safe value of the collateral. Both of these conditions are conditioned by the need to limit potential losses. And the second is also conditioned by the low liquidity of atypical objects, including those whose absolute or unit price is too high. In addition, insurance companies usually have their own limits, and they would not accept the risk of an excessive insurance payout, shifting it to the insured in the part of exceeding the limit. Therefore, it is reasonable to set the upper limit of the mortgaged property. In this case, the property with a value lower than the limit would be accepted as collateral for its full value. And a property with a value higher than the limit would be accepted at the limit value. This applies to both total and unit values.\n",
    "\n",
    "In order to achieve a balance between financial security and profitability, the best choice is to determine the market values of safe limits. This task requires estimation of certain specific parameters of the whole market. Since we have only a sample, we should apply some special methods that will help us to correct the bias between sample and population estimates. Now we can create a system of rules. \n",
    "1. The upper limit of the total value of the property is equal to the expectation plus one standard deviation.\n",
    "1. The upper limit of the unit value of the property is equal to the expectation plus two standard deviations.\n",
    "\n",
    "The specific values of the standard deviation could be shifted in one direction or the other, depending on a bank's objectives.\n",
    "\n",
    "Then we will estimate the values of the necessary parameters. We should estimate the true values of the four parameters: total and unit market values, as well as their standard deviations. As mentioned above, we can't get true values directly because we're working with samples. Therefore, we will use the method of bootstrapping, which allows us to reduce the bias of the sample estimates relative to the true values that exist in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b4802",
   "metadata": {},
   "source": [
    "## Description of the method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ee5ea8",
   "metadata": {},
   "source": [
    "### The General Statements of Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de465862",
   "metadata": {},
   "source": [
    "**Bootstrapping** is a random sampling with replacement technique. It belongs to the broader class of resampling methods. This technique can be used to estimate the sampling distribution of almost any statistic.\n",
    "\n",
    "In practice, when analyzing data, appraisers are almost always faced with the need to estimate the variance and confidence intervals of some measure. It is difficult to make useful inferences based on point estimates alone. Therefore, it is very useful to be able to estimate the confidence intervals of any indicators. Classical statistics tells us that we can, for example, estimate confidence intervals for the mean under certain assumptions. But it does not give a simple answer to the question of what to do if these assumptions are violated. Estimating confidence intervals for the median is a much more complex problem in itself. So we need a universal method that allows us to estimate almost any parameter and obtain its variance and confidence intervals regardless of any assumptions about the distribution of that parameter.\n",
    "\n",
    "The method of bootstrapping successfully solves the problem of obtaining values of such indicators regardless of the data distribution. Moreover, it allows you to improve any sample estimates, bringing their values closer to the true values in the population.\n",
    "\n",
    "Let's imagine that there is some general population that has N elements. But we only have a sample of size n. If the N and n are the same, we can just get any parameter directly from the sample. In practice, the n is just almost far smaller than the N. So, we need to apply some function 'F(X)' to obtain estimates of some parameter for the sample. In the case where we know the distribution of the parameter in the general population, we can use the obtained point estimate. But more often than not, we do not know this distribution. In this case, the resulting point estimate is not useful enough for us. The fact is that we cannot somehow correlate it with the true value of the parameter for the entire general population. So we need to somehow get not only the estimate, but also its expected distribution in the population. That is, we should get a set of estimates as if we had many samples. But we only have one sample. So we need some kind of resampling. The summary of the above is shown in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b03c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"756pt\" height=\"364pt\"\n",
       " viewBox=\"0.00 0.00 756.12 364.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 360)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-360 752.12,-360 752.12,4 -4,4\"/>\n",
       "<!-- n_6d984edae9e042a7b35128a700f18be9 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>n_6d984edae9e042a7b35128a700f18be9</title>\n",
       "<ellipse fill=\"lightblue\" stroke=\"black\" cx=\"170.93\" cy=\"-338\" rx=\"170.87\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"170.93\" y=\"-334.3\" font-family=\"Times,serif\" font-size=\"14.00\">Population: N&gt;&gt;1, Sample: n&lt;&lt;N</text>\n",
       "</g>\n",
       "<!-- n_b0ab7c69ef3c4f1ab73864bdb3d894be -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>n_b0ab7c69ef3c4f1ab73864bdb3d894be</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"170.93,-283 102.9,-265 170.93,-247 238.97,-265 170.93,-283\"/>\n",
       "<text text-anchor=\"middle\" x=\"170.93\" y=\"-261.3\" font-family=\"Times,serif\" font-size=\"14.00\">Is n=N?</text>\n",
       "</g>\n",
       "<!-- n_6d984edae9e042a7b35128a700f18be9&#45;&gt;n_b0ab7c69ef3c4f1ab73864bdb3d894be -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>n_6d984edae9e042a7b35128a700f18be9&#45;&gt;n_b0ab7c69ef3c4f1ab73864bdb3d894be</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M170.93,-319.81C170.93,-311.79 170.93,-302.05 170.93,-293.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"174.43,-293.03 170.93,-283.03 167.43,-293.03 174.43,-293.03\"/>\n",
       "</g>\n",
       "<!-- n_44773b47d8974288afdae218dbd83840 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>n_44773b47d8974288afdae218dbd83840</title>\n",
       "<polygon fill=\"Red\" stroke=\"black\" points=\"269.25,-170.54 269.25,-185.46 197.6,-196 96.27,-196 24.62,-185.46 24.62,-170.54 96.27,-160 197.6,-160 269.25,-170.54\"/>\n",
       "<text text-anchor=\"middle\" x=\"146.93\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"white\">Obtain values directly</text>\n",
       "</g>\n",
       "<!-- n_b0ab7c69ef3c4f1ab73864bdb3d894be&#45;&gt;n_44773b47d8974288afdae218dbd83840 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>n_b0ab7c69ef3c4f1ab73864bdb3d894be&#45;&gt;n_44773b47d8974288afdae218dbd83840</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.42,-248.01C163.09,-236.21 158.47,-219.86 154.57,-206.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"157.84,-204.76 151.76,-196.08 151.11,-206.66 157.84,-204.76\"/>\n",
       "<text text-anchor=\"middle\" x=\"173.43\" y=\"-217.8\" font-family=\"Times,serif\" font-size=\"14.00\">Yes</text>\n",
       "</g>\n",
       "<!-- n_48b943c65cb04c6ca477e3fc70914eba -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>n_48b943c65cb04c6ca477e3fc70914eba</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"443.93,-196 287.92,-178 443.93,-160 599.95,-178 443.93,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"443.93\" y=\"-174.3\" font-family=\"Times,serif\" font-size=\"14.00\">Distribution is known</text>\n",
       "</g>\n",
       "<!-- n_b0ab7c69ef3c4f1ab73864bdb3d894be&#45;&gt;n_48b943c65cb04c6ca477e3fc70914eba -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>n_b0ab7c69ef3c4f1ab73864bdb3d894be&#45;&gt;n_48b943c65cb04c6ca477e3fc70914eba</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M200.67,-254.74C247.08,-240.29 337.27,-212.21 393.84,-194.6\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"395.14,-197.86 403.65,-191.54 393.06,-191.17 395.14,-197.86\"/>\n",
       "<text text-anchor=\"middle\" x=\"334.43\" y=\"-217.8\" font-family=\"Times,serif\" font-size=\"14.00\">No</text>\n",
       "</g>\n",
       "<!-- n_75c256bc34fe44d1947ebdd93b6a249e -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>n_75c256bc34fe44d1947ebdd93b6a249e</title>\n",
       "<polygon fill=\"Red\" stroke=\"black\" points=\"486.31,-83.54 486.31,-98.46 416.97,-109 318.9,-109 249.56,-98.46 249.56,-83.54 318.9,-73 416.97,-73 486.31,-83.54\"/>\n",
       "<text text-anchor=\"middle\" x=\"367.93\" y=\"-87.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"white\">Apply any estimators</text>\n",
       "</g>\n",
       "<!-- n_48b943c65cb04c6ca477e3fc70914eba&#45;&gt;n_75c256bc34fe44d1947ebdd93b6a249e -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>n_48b943c65cb04c6ca477e3fc70914eba&#45;&gt;n_75c256bc34fe44d1947ebdd93b6a249e</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M430,-161.41C418.86,-148.95 403.04,-131.26 390.16,-116.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"392.63,-114.37 383.35,-109.25 387.41,-119.03 392.63,-114.37\"/>\n",
       "<text text-anchor=\"middle\" x=\"423.43\" y=\"-130.8\" font-family=\"Times,serif\" font-size=\"14.00\">Yes</text>\n",
       "</g>\n",
       "<!-- n_1895b5955f59443283f43483363acc22 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>n_1895b5955f59443283f43483363acc22</title>\n",
       "<ellipse fill=\"lightblue\" stroke=\"black\" cx=\"625.93\" cy=\"-91\" rx=\"122.38\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"625.93\" y=\"-87.3\" font-family=\"Times,serif\" font-size=\"14.00\">Apply &#39;F(X)&#39; for n* times</text>\n",
       "</g>\n",
       "<!-- n_48b943c65cb04c6ca477e3fc70914eba&#45;&gt;n_1895b5955f59443283f43483363acc22 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>n_48b943c65cb04c6ca477e3fc70914eba&#45;&gt;n_1895b5955f59443283f43483363acc22</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M473.18,-163.34C502.48,-149.66 547.95,-128.42 581.67,-112.67\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"583.27,-115.79 590.85,-108.39 580.3,-109.45 583.27,-115.79\"/>\n",
       "<text text-anchor=\"middle\" x=\"555.43\" y=\"-130.8\" font-family=\"Times,serif\" font-size=\"14.00\">No</text>\n",
       "</g>\n",
       "<!-- n_289b58e1dcc0421e9b5f72dd808c6cbd -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>n_289b58e1dcc0421e9b5f72dd808c6cbd</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"712.43,-36 539.43,-36 539.43,0 712.43,0 712.43,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"625.93\" y=\"-14.3\" font-family=\"Courier,monospace\" font-size=\"14.00\">Obtain didtribution</text>\n",
       "</g>\n",
       "<!-- n_1895b5955f59443283f43483363acc22&#45;&gt;n_289b58e1dcc0421e9b5f72dd808c6cbd -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>n_1895b5955f59443283f43483363acc22&#45;&gt;n_289b58e1dcc0421e9b5f72dd808c6cbd</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" d=\"M625.93,-72.81C625.93,-61.65 625.93,-47.16 625.93,-36.03\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<flowgiston.base.FlowgistonChart at 0x7f1791bb3ee0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flowgiston import *\n",
    "Base = flowgiston_base(fillcolor='lightblue')\n",
    "class Yes(Base):\n",
    "    fillcolor = 'Red'\n",
    "    fontcolor = 'white'\n",
    "    shape = 'octagon'\n",
    "\n",
    "class Note(Base):\n",
    "    fillcolor = 'lightyellow'\n",
    "    style = 'filled,dashed'\n",
    "    shape = 'box'\n",
    "    fontname = 'courier'\n",
    "\n",
    "chart = FlowgistonChart(Base)\n",
    "\n",
    "sample = chart.start(\"Population: N>>1, Sample: n<<N\").edge(chart.if_(\"Is n=N?\"))\n",
    "sample.yes(chart.Yes.node(\"Obtain values directly\"))\n",
    "distribution = sample.no(chart.if_(\"Distribution is known\"))\n",
    "distribution.yes(chart.Yes.node(\"Apply any estimators\"))\n",
    "end = distribution.no(chart.end(\"Apply 'F(X)' for n* times\")).edge(chart.Note.node(\"Obtain didtribution\"), style='dashed', dir='none')\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c3b17b",
   "metadata": {},
   "source": [
    "Let's look at an example. We need to find the distribution of the following parameter\n",
    "$$\\frac{\\overline{x}-\\mu}{\\frac{S}{\\sqrt{n}}}.$$\n",
    "\n",
    "Based on the flowchart above, we are forced to reject direct estimation. Then we can choose between the classical statistical approach and resampling. The first requires that we know the distribution. Or we can make some assumptions about the distribution. For example, we can assume that the estimated characteristic has a normal distribution in the general population. Then our parameter would have the t-distribution. But we have no answer to what would happen if our assumptions were wrong.\n",
    "\n",
    "In valuation practice, we almost always do not know the distribution of a feature in the market and cannot make reliable guesses about it. Bootstrapping can help us in such a case. The idea is as follows. We have only one sample. Therefore, we can only get the point estimate of the parameter. But if we were able to get some number of samples from the general population, we would also be able to get the distribution of the parameter in them. This, in turn, would allow us to obtain the confidence intervals of the parameter, regardless of any assumptions. In practice, we are not able to get new samples without losses in size. But we can simulate new pseudo-samples as if we could get some samples from the population.\n",
    "\n",
    "If our sample is large enough, its distribution is to some extent a reflection of the distribution in the general population. This fact allows us to make an important step in understanding the concept of bootstrapping, as well as the general concept of resampling. The classical statistical approach relies on a prior assumption about the shape of the distribution in the general population. Resampling methods, on the other hand, assume that the distribution in the general population is approximately the same as the sampling distribution. In a more general sense, we can say that the method involves working with the sample as if it were the general population. We take a sample and assume that it reflects the distribution of the population. Then we apply some resampling techniques. Then we use the function to calculate the statistics we are interested in. As a result, we extend the conclusions drawn from the sample to the entire population. The described technique looks a bit recursive. However, it works very well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f17f1bb",
   "metadata": {},
   "source": [
    "### The technique of bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bce9646",
   "metadata": {},
   "source": [
    "Let's consider a simple case, the more advanced version of which will be useful for our main task. We have a sample $X[x_{1}\\ldots x_{n}]$ and want to find the confidence interval for the mean. Of course, we have no problem calculating the sample mean. However, it's just a single number that relates only to the sample itself. The classical approach tells us that we can get the confidence interval from this value. But only if the distribution is known. In practice, we have no idea about the true distribution in the market. Therefore, it is very likely that the calculated confidence interval will be misleading. Instead, we will assume that the sample is fairly representative and to some extent reflects the true distribution in the population. Next, we will take the following steps.\n",
    "1. We take our sample and use random sampling with replacement to create a new sample of the same size. This means that one or more observations from the original sample can be repeated any number of times in the new sample.\n",
    "1. Repeat the previous step at least a thousand times. The more times you repeat this process, the better the result. Thus, the upper limit of repetitions is set only by the computing power.\n",
    "1. Let's introduce some new notation and label the new sample as $X^{*}$. So we now have a set of $X^{*}$ from $X_{1}^{*}$ to $X_{B}^{*}$.\n",
    "1. Calculate the statistic of interest for each $X^{*}$. In our case, it will be the mean. This will give us a set of values $\\overline{X_{1}^{*}},\\ldots,\\overline{X_{B}^{*}}$.\n",
    "1. It is easy to see that we now have the distribution of the statistic of interest. This allows us to work with it as we would with any known distribution. For example, we can set quantiles of interest and calculate the confidence interval, which was our original goal.\n",
    "\n",
    "The above algorithm allows us to provide an implementation of the **percentile bootstrap**. It is the simplest and most intuitive type of bootstrapping. But it has some limitations. The main one is that this technique can only handle symmetric distributions well. The more sophisticated but also more universal technique is the **basic bootstrap**. This approach uses a slightly different technique. It has the same first two steps. But in the third step we calculate a different statistic:$\\overline{X} - \\overline{X_{i}^{*}}$. As a result, we get the distribution of the following statistic\n",
    "$$\\overline{X} - \\overline{X_{i}^{*}}.$$\n",
    "Its main idea is that we should achieve the following due to a very large number of repeated samples \n",
    "$$\\mu - \\overline{X} \\approx \\overline{X} - \\overline{X^{*}}.$$\n",
    "Let's label the distribution of $\\mu - \\overline{X}$ as $\\beta$. If we knew alpha, we could get its quantiles, for example $\\beta_{0.025}, \\beta_{0.975}$. This means that we could estimate the probability of the following\n",
    "$$\n",
    "P\\left(\\mu-(\\overline{x}) \\in [\\beta_{0.025}, \\beta_{0.975}]\\right)\n",
    "$$\n",
    "Of course, for the known beta, the probability is 0.95. The previous equation can be rewritten as follows\n",
    "$$\\begin{cases}\n",
    "\\mu-\\overline{x} \\leq \\beta_{0.975}\\\\\n",
    "\\mu-\\overline{x} \\geq \\beta_{0.025}.\n",
    "\\end{cases}\n",
    "$$\n",
    "It gives us the knowledge that\n",
    "$$\n",
    "P\\left(\\mu \\in [\\beta_{0.025}+\\overline{x}, \\beta_{0.975}++\\overline{x}]\\right).\n",
    "$$\n",
    "Thus, we obtain the distribution of the population by bootstrap approximation and estimate the confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a430cea",
   "metadata": {},
   "source": [
    "#### Testing hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9070a3ed",
   "metadata": {},
   "source": [
    "The universality of bootstrapping is that it allows not only to obtain a statistic, but also to test hypotheses. Let us consider an example. Let us have two samples: $X=x_{1},\\ldots,x_{n}$ and $Y=y_{1},\\ldots,y_{m}$. And we want to test if the expectations of X and Y are equal: $\\mu_{X}=\\mu_{Y}\\ -\\ ?$. However, all we can get are the sample means. The well-known t-test can do the job, but it also has a well-known limitation. It is based on the assumption of a normal distribution of the characteristic in both samples. However, our distributions are unknown. Bootstrapping will help us to overcome this limitation. The basic approach is the same as in classical statistics. We construct null and alternative hypotheses. Then we estimate the distribution of the resulting statistic for the case where the null hypothesis is true. But the hypotheses themselves are a bit exotic. The null hypothesis is that the distributions of the *X* and *Y* are equal.\n",
    "1. Combine the two samples into one, keeping the data about the origin of each observation.\n",
    "1. Generate new sample of size *m+n* by resampling with replacement.\n",
    "1. Divide it into two samples by the trait of origin.\n",
    "1. Calculate $\\overline{X_{i}^{*}}$ and $\\overline{Y_{j}^{*}}$.\n",
    "1. Subtract $\\overline{Y_{j}^{*}}$ from $\\overline{X_{i}^{*}}$.\n",
    "1. Repeat steps 2 through 5 *k* times *(k>1000)*.\n",
    "1. Now we have the distribution of the differences between $X^{*}$ and $Y^{*}$.\n",
    "1. All we need now is to subtract Y from X and find the probability *(p-value)* of such a difference for our distribution obtained in the previous step.\n",
    "1. Next, we compare the p-value to the $\\alpha$ (the most common is 0.05) and make a decision whether to reject the null hypothesis. \n",
    "The last two steps are no different from the classical approach to hypothesis testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4528a2fa",
   "metadata": {},
   "source": [
    "## Practical implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5b7e6",
   "metadata": {},
   "source": [
    "Let's return to our learning task and compute the true values in the market to help the bank design its credit policy. We will use the real data set with 2355 observations in one of the markets. Today we have the two columns of interest: price and price_m. The first one contains data on the prices of the real estate objects themselves, and the second one on their prices per square meter. We will use this data set as the general population to test the ability of bootstrapping to improve the sample estimates. We will take a sample from the entire dataset and use it as a real sample that might be available in the collateral department of a bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ad99cb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimation of price for population is 23372356.69\n",
      "The estimation of unit price for population is 361554.02\n",
      "The standard deviation of price for population is 16061866.51\n",
      "The standard deviation of unit price for population is 95947.20\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# limitation of digits\n",
    "# pd.set_option('display.float_format', lambda x: f'{x:.5f}')\n",
    "# np.set_printoptions(precision=5)\n",
    "\n",
    "# import data\n",
    "data = pd.read_csv(\"ds.csv\", index_col=False)\n",
    "\n",
    "# calculate statistics for the \"general population\"\n",
    "gp_est_tp    = data['price'].mean()\n",
    "gp_est_up    = data['price_m'].mean()\n",
    "gp_std_tp    = data['price'].std()\n",
    "gp_std_up    = data['price_m'].std()\n",
    "formatted_gp_est_tp = '{:.2f}'.format(gp_est_tp)\n",
    "formatted_gp_est_up = '{:.2f}'.format(gp_est_up)\n",
    "formatted_gp_std_tp = '{:.2f}'.format(gp_std_tp)\n",
    "formatted_gp_std_up = '{:.2f}'.format(gp_std_up)\n",
    "print(\"The estimation of price for population is\", formatted_gp_est_tp)\n",
    "print(\"The estimation of unit price for population is\", formatted_gp_est_up)\n",
    "print(\"The standard deviation of price for population is\", formatted_gp_std_tp)\n",
    "print(\"The standard deviation of unit price for population is\", formatted_gp_std_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a378c",
   "metadata": {},
   "source": [
    "Now we have the \"true\" values in the market. Let's create a sample of modest size, so far characteristic of valuation practice. Next, we calculate the same statistics for the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "28ec3db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean price for sample is 22520000.00\n",
      "The mean unit price for sample is 348620.68\n",
      "The standard deviation of price for sample is 14239488.41\n",
      "The standard deviation of unit price for sample is 94539.94\n"
     ]
    }
   ],
   "source": [
    "# create sample\n",
    "sample_size = 100\n",
    "rand_sample = data.sample(n=sample_size)\n",
    "\n",
    "# calculate statistics for the sample\n",
    "sam_mean_tp    = rand_sample['price'].mean()\n",
    "sam_mean_up    = rand_sample['price_m'].mean()\n",
    "sam_std_tp    = rand_sample['price'].std()\n",
    "sam_std_up    = rand_sample['price_m'].std()\n",
    "formatted_sam_mean_tp = '{:.2f}'.format(sam_mean_tp)\n",
    "formatted_sam_mean_up = '{:.2f}'.format(sam_mean_up)\n",
    "formatted_sam_std_tp = '{:.2f}'.format(sam_std_tp)\n",
    "formatted_sam_std_up = '{:.2f}'.format(sam_std_up)\n",
    "print(\"The mean price for sample is\", formatted_sam_mean_tp)\n",
    "print(\"The mean unit price for sample is\", formatted_sam_mean_up)\n",
    "print(\"The standard deviation of price for sample is\", formatted_sam_std_tp)\n",
    "print(\"The standard deviation of unit price for sample is\", formatted_sam_std_up)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23497e7",
   "metadata": {},
   "source": [
    "Now we calculate the deviations of the sample statistics from the true ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "2da40336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The absolute deviation of the price mean is 852356.68790\n",
      "The absolute deviation of the unit price mean is 12933.33656\n",
      "The absolute deviation of the price standard deviation is 1822378.09698\n",
      "The absolute deviation of the unit price standard deviation is 1407.26742\n",
      "The relative deviation of the price mean is 0.03647\n",
      "The relative deviation of the unit price mean is 0.03577\n",
      "The relative deviation of the price standard deviation is 0.11346\n",
      "The relative deviation of the unit price standard deviation is 0.01467\n"
     ]
    }
   ],
   "source": [
    "tp_mean_dev_abs = abs(sam_mean_tp - gp_est_tp)\n",
    "up_mean_dev_abs = abs(sam_mean_up - gp_est_up)\n",
    "tp_std_dev_abs  = abs(sam_std_tp  - gp_std_tp)\n",
    "up_std_dev_abs  = abs(sam_std_up  - gp_std_up)\n",
    "tp_mean_dev_rel = abs(sam_mean_tp - gp_est_tp) / gp_est_tp\n",
    "up_mean_dev_rel = abs(sam_mean_up - gp_est_up) / gp_est_up\n",
    "tp_std_dev_rel  = abs(sam_std_tp  - gp_std_tp) / gp_std_tp\n",
    "up_std_dev_rel  = abs(sam_std_up  - gp_std_up) / gp_std_up\n",
    "\n",
    "formatted_tp_mean_dev_abs = '{:.5f}'.format(tp_mean_dev_abs)\n",
    "formatted_up_mean_dev_abs = '{:.5f}'.format(up_mean_dev_abs)\n",
    "formatted_tp_std_dev_abs  = '{:.5f}'.format(tp_std_dev_abs)\n",
    "formatted_up_std_dev_abs  = '{:.5f}'.format(up_std_dev_abs)\n",
    "formatted_tp_mean_dev_rel = '{:.5f}'.format(tp_mean_dev_rel)\n",
    "formatted_up_mean_dev_rel = '{:.5f}'.format(up_mean_dev_rel)\n",
    "formatted_tp_std_dev_rel  = '{:.5f}'.format(tp_std_dev_rel)\n",
    "formatted_up_std_dev_rel  = '{:.5f}'.format(up_std_dev_rel)\n",
    "\n",
    "print(\"The absolute deviation of the price mean is\", formatted_tp_mean_dev_abs)\n",
    "print(\"The absolute deviation of the unit price mean is\", formatted_up_mean_dev_abs)\n",
    "print(\"The absolute deviation of the price standard deviation is\", formatted_tp_std_dev_abs)\n",
    "print(\"The absolute deviation of the unit price standard deviation is\", formatted_up_std_dev_abs)\n",
    "print(\"The relative deviation of the price mean is\", formatted_tp_mean_dev_rel)\n",
    "print(\"The relative deviation of the unit price mean is\", formatted_up_mean_dev_rel)\n",
    "print(\"The relative deviation of the price standard deviation is\", formatted_tp_std_dev_rel)\n",
    "print(\"The relative deviation of the unit price standard deviation is\", formatted_up_std_dev_rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b202ff49",
   "metadata": {},
   "source": [
    "Now we can move on to bootstrapping. There are some library functions for this, but today we will create our own function to better understand the nature of the technique. It implements the basic way, for more sophisticated algorithms like smoothed bootstrap or Bayesian bootstrap you should use functions from libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "97da43a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of means:  22497706.0\n",
      "Standard deviation of means:  1417790.8095216304\n",
      "Mean of standard deviations:  13852963.114257274\n",
      "Standard deviation of standard deviations:  2280844.4736604476\n"
     ]
    }
   ],
   "source": [
    "# Price\n",
    "\n",
    "# Define the number of bootstrap samples to take\n",
    "num_bootstraps = 1000\n",
    "\n",
    "# Initialize arrays to store the bootstrapped means and standard deviations\n",
    "bootstrapped_means = np.zeros(num_bootstraps)\n",
    "bootstrapped_stdevs = np.zeros(num_bootstraps)\n",
    "\n",
    "# Perform the bootstrapping\n",
    "for i in range(num_bootstraps):\n",
    "    bootstrap_sample = np.random.choice(rand_sample['price'], size=len(rand_sample), replace=True)\n",
    "    bootstrapped_means[i] = np.mean(bootstrap_sample)\n",
    "    bootstrapped_stdevs[i] = np.std(bootstrap_sample)\n",
    "\n",
    "# Calculate the mean and standard deviation of the bootstrapped means and standard deviations\n",
    "tp_mean_of_means = np.mean(bootstrapped_means)\n",
    "tp_std_of_means = np.std(bootstrapped_means)\n",
    "tp_mean_of_stdevs = np.mean(bootstrapped_stdevs)\n",
    "tp_std_of_stdevs = np.std(bootstrapped_stdevs)\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean of means: \", tp_mean_of_means)\n",
    "print(\"Standard deviation of means: \", tp_std_of_means)\n",
    "print(\"Mean of standard deviations: \", tp_mean_of_stdevs)\n",
    "print(\"Standard deviation of standard deviations: \", tp_std_of_stdevs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f463f6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of means:  348935.11415999994\n",
      "Standard deviation of means:  9441.378020175078\n",
      "Mean of standard deviations:  93215.91826463927\n",
      "Standard deviation of standard deviations:  11701.248943112578\n"
     ]
    }
   ],
   "source": [
    "# Unit price\n",
    "\n",
    "# Define the number of bootstrap samples to take\n",
    "num_bootstraps = 1000\n",
    "\n",
    "# Initialize arrays to store the bootstrapped means and standard deviations\n",
    "bootstrapped_means = np.zeros(num_bootstraps)\n",
    "bootstrapped_stdevs = np.zeros(num_bootstraps)\n",
    "\n",
    "# Perform the bootstrapping\n",
    "for i in range(num_bootstraps):\n",
    "    bootstrap_sample = np.random.choice(rand_sample['price_m'], size=len(rand_sample), replace=True)\n",
    "    bootstrapped_means[i] = np.mean(bootstrap_sample)\n",
    "    bootstrapped_stdevs[i] = np.std(bootstrap_sample)\n",
    "\n",
    "# Calculate the mean and standard deviation of the bootstrapped means and standard deviations\n",
    "up_mean_of_means = np.mean(bootstrapped_means)\n",
    "up_std_of_means = np.std(bootstrapped_means)\n",
    "up_mean_of_stdevs = np.mean(bootstrapped_stdevs)\n",
    "up_std_of_stdevs = np.std(bootstrapped_stdevs)\n",
    "\n",
    "# Print the results\n",
    "print(\"Mean of means: \", up_mean_of_means)\n",
    "print(\"Standard deviation of means: \", up_std_of_means)\n",
    "print(\"Mean of standard deviations: \", up_mean_of_stdevs)\n",
    "print(\"Standard deviation of standard deviations: \", up_std_of_stdevs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f46ae19",
   "metadata": {},
   "source": [
    "Now we can estimate the performance of bootstrapping. Let's calculate the decrease of bias of the sample estimates relate to the true ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "f88e3ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The absolute deviation of the bootstrapped price mean is 874650.68790\n",
      "The absolute deviation of the bootstrapped unit price mean is 12618.90240\n",
      "The absolute deviation of the bootstrapped price standard deviation is 2208903.39710\n",
      "The absolute deviation of the bootstrapped unit price standard deviation is 2731.28507\n",
      "The relative deviation of the bootstrapped price mean is 0.03742\n",
      "The relative deviation of the bootstrapped unit price mean is 0.03490\n",
      "The relative deviation of the bootstrapped price standard deviation is 0.13752\n",
      "The relative deviation of the bootstrapped unit price standard deviation is 0.02847\n"
     ]
    }
   ],
   "source": [
    "b_tp_mean_dev_abs = abs(tp_mean_of_means - gp_est_tp)\n",
    "b_up_mean_dev_abs = abs(up_mean_of_means - gp_est_up)\n",
    "b_tp_std_dev_abs  = abs(tp_mean_of_stdevs  - gp_std_tp)\n",
    "b_up_std_dev_abs  = abs(up_mean_of_stdevs  - gp_std_up)\n",
    "b_tp_mean_dev_rel = abs(tp_mean_of_means - gp_est_tp) / gp_est_tp\n",
    "b_up_mean_dev_rel = abs(up_mean_of_means - gp_est_up) / gp_est_up\n",
    "b_tp_std_dev_rel  = abs(tp_mean_of_stdevs  - gp_std_tp) / gp_std_tp\n",
    "b_up_std_dev_rel  = abs(up_mean_of_stdevs  - gp_std_up) / gp_std_up\n",
    "\n",
    "formatted_b_tp_mean_dev_abs = '{:.5f}'.format(b_tp_mean_dev_abs)\n",
    "formatted_b_up_mean_dev_abs = '{:.5f}'.format(b_up_mean_dev_abs)\n",
    "formatted_b_tp_std_dev_abs  = '{:.5f}'.format(b_tp_std_dev_abs)\n",
    "formatted_b_up_std_dev_abs  = '{:.5f}'.format(b_up_std_dev_abs)\n",
    "formatted_b_tp_mean_dev_rel = '{:.5f}'.format(b_tp_mean_dev_rel)\n",
    "formatted_b_up_mean_dev_rel = '{:.5f}'.format(b_up_mean_dev_rel)\n",
    "formatted_b_tp_std_dev_rel  = '{:.5f}'.format(b_tp_std_dev_rel)\n",
    "formatted_b_up_std_dev_rel  = '{:.5f}'.format(b_up_std_dev_rel)\n",
    "\n",
    "print(\"The absolute deviation of the bootstrapped price mean is\", formatted_b_tp_mean_dev_abs)\n",
    "print(\"The absolute deviation of the bootstrapped unit price mean is\", formatted_b_up_mean_dev_abs)\n",
    "print(\"The absolute deviation of the bootstrapped price standard deviation is\", formatted_b_tp_std_dev_abs)\n",
    "print(\"The absolute deviation of the bootstrapped unit price standard deviation is\", formatted_b_up_std_dev_abs)\n",
    "print(\"The relative deviation of the bootstrapped price mean is\", formatted_b_tp_mean_dev_rel)\n",
    "print(\"The relative deviation of the bootstrapped unit price mean is\", formatted_b_up_mean_dev_rel)\n",
    "print(\"The relative deviation of the bootstrapped price standard deviation is\", formatted_b_tp_std_dev_rel)\n",
    "print(\"The relative deviation of the bootstrapped unit price standard deviation is\", formatted_b_up_std_dev_rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9c14b245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The improvement of the price mean estimate is -0.02616\n",
      "The improvement of the unit price mean estimate is 0.02431\n",
      "The improvement of the price standard deviation is -0.21210\n",
      "The improvement of the unit price standard deviation is -0.94084\n"
     ]
    }
   ],
   "source": [
    "tp_price_improve_mean = (tp_mean_dev_abs - b_tp_mean_dev_abs) / tp_mean_dev_abs\n",
    "up_price_improve_mean = (up_mean_dev_abs - b_up_mean_dev_abs) / up_mean_dev_abs\n",
    "tp_price_improve_std = (tp_std_dev_abs - b_tp_std_dev_abs) / tp_std_dev_abs\n",
    "up_price_improve_std = (up_std_dev_abs - b_up_std_dev_abs) / up_std_dev_abs\n",
    "\n",
    "f_tp_price_improve_mean = '{:.5f}'.format(tp_price_improve_mean)\n",
    "f_up_price_improve_mean = '{:.5f}'.format(up_price_improve_mean)\n",
    "f_tp_price_improve_std  = '{:.5f}'.format(tp_price_improve_std)\n",
    "f_up_price_improve_std  = '{:.5f}'.format(up_price_improve_std)\n",
    "\n",
    "print(\"The improvement of the price mean estimate is\", f_tp_price_improve_mean)\n",
    "print(\"The improvement of the unit price mean estimate is\", f_up_price_improve_mean)\n",
    "print(\"The improvement of the price standard deviation is\", f_tp_price_improve_std)\n",
    "print(\"The improvement of the unit price standard deviation is\", f_up_price_improve_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "95e1243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothed_bootstrap(data, n_bootstraps=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Computes smoothed bootstrap for mean and standard deviation of data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        The data to be analyzed.\n",
    "    n_bootstraps : int, optional\n",
    "        The number of bootstrap samples to generate. Default is 1000.\n",
    "    alpha : float, optional\n",
    "        The significance level used for computing confidence intervals. Default is 0.05.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    A tuple containing:\n",
    "        - The smoothed bootstrap mean and its confidence interval.\n",
    "        - The smoothed bootstrap standard deviation and its confidence interval.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    bootstrap_means = []\n",
    "    bootstrap_stds = []\n",
    "    for i in range(n_bootstraps):\n",
    "        sample_indices = np.random.choice(range(n), size=n, replace=True)\n",
    "        bootstrap_mean = np.mean(data[sample_indices])\n",
    "        bootstrap_std = np.std(data[sample_indices])\n",
    "        bootstrap_means.append(bootstrap_mean)\n",
    "        bootstrap_stds.append(bootstrap_std)\n",
    "    \n",
    "    smoothed_means = []\n",
    "    smoothed_stds = []\n",
    "    for i in range(n_bootstraps):\n",
    "        kernel_weights = np.exp(-(i - np.arange(n_bootstraps))**2 / (2 * (alpha * n_bootstraps)**2))\n",
    "        smoothed_mean = np.sum(kernel_weights * bootstrap_means) / np.sum(kernel_weights)\n",
    "        smoothed_std = np.sum(kernel_weights * bootstrap_stds) / np.sum(kernel_weights)\n",
    "        smoothed_means.append(smoothed_mean)\n",
    "        smoothed_stds.append(smoothed_std)\n",
    "    \n",
    "    mean_ci = np.percentile(smoothed_means, [100 * alpha / 2, 100 * (1 - alpha / 2)])\n",
    "    std_ci = np.percentile(smoothed_stds, [100 * alpha / 2, 100 * (1 - alpha / 2)])\n",
    "    return (np.mean(smoothed_means), mean_ci), (np.mean(smoothed_stds), std_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "fe605cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothed bootstrap mean: 22508265.99 (22384352.88, 22632698.41)\n",
      "Smoothed bootstrap standard deviation: 13930150.55 (13736116.25, 14208091.73)\n"
     ]
    }
   ],
   "source": [
    "price   = rand_sample['price']\n",
    "price = price.values\n",
    "mean, std = smoothed_bootstrap(price)\n",
    "print(f\"Smoothed bootstrap mean: {mean[0]:.2f} ({mean[1][0]:.2f}, {mean[1][1]:.2f})\")\n",
    "print(f\"Smoothed bootstrap standard deviation: {std[0]:.2f} ({std[1][0]:.2f}, {std[1][1]:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c64669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
