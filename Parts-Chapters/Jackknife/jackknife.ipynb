{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "088c3024",
   "metadata": {},
   "source": [
    "# Overcoming the problem of biased estimates in the analysis of open market data with the Jackknife resampling method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7439b31e",
   "metadata": {},
   "source": [
    "Cyrill A. Murashev, 2023-02-05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ef988",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d354f2",
   "metadata": {},
   "source": [
    "Appraisers are often faced with the need to analyze and describe market data collected in open markets. Almost always, they can't get the data for the whole market, but they are dealing with samples that may be very small compared to the whole population. In this case, the problem of biased estimates arises. It follows from the above that any statistical estimate made on the basis of the sample in question is an estimate for the sample itself. At the same time, it may have a bias relative to the estimate that would be obtained in the case of an analysis of the entire general population. Appraisers often say that they have calculated some descriptive statistics of the market. It may be the price mean or median, maximum and minimum, etc. But we should understand that these are only estimates for samples, not for the entire market. Today we will look at the minimal theoretical basis of the method. And then we will implement it on real market data using the Python language. We will learn how to determine whether bias exists for any estimate and how to automatically reduce its linear component. This paper is available in [English](https://github.com/Kirill-Murashev/AI_for_valuers_book/blob/main/Parts-Chapters/Jackknife/jackknife.ipynb), [Spanish](https://github.com/Kirill-Murashev/AI_for_valuers_book/blob/main/Parts-Chapters/Jackknife/jackknife-esp.ipynb), and [Russian](https://github.com/Kirill-Murashev/AI_for_valuers_book/blob/main/Parts-Chapters/Jackknife/jackknife-nov.ipynb). The English version is the most current and the most quickly updated. If there are any discrepancies between the versions, the English version should be relied upon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc949a47",
   "metadata": {},
   "source": [
    "## Fundamentals of the Jackknife Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0241824a",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1676a826",
   "metadata": {},
   "source": [
    "First, we need to remember why appraisers need statistics. Usually they have some distribution of features of objects from the sample of analogues collected on the open market. And they try to get some estimates of the values of these characteristics. It can be mean, median, maximum, minimum, variance, etc. Sometimes they also need to compare two or more subsamples to decide if some adjustments are needed based on the difference of the feature values. As we can guess, most of the time appraisers are dealing with samples, not the entire market. Thus, appraisers can only obtain sample estimates of the feature values, not their true values. \n",
    "The jackknife method can address two issues:\n",
    "- reduce the bias of the sample estimate relative to the true value from the general population;\n",
    "- calculate the variance of the adjusted trait value.\n",
    "\n",
    "Suppose we have some characteristic *X* (it could be the unit price, for example), the distribution of which in the general population is unknown to us. But we have a sample consisting of n elements $[x{1},\\ldots, x_{n}]$. We want to estimate the expectation of *X*, which can be written as $\\mathbb{E}[X]$. In general, the expectation can be written as follows\n",
    "$$\\mathbb{E}[X] = \\sum_{j=1}^{n >> 1} p(x_{j})x_{j}.$$\n",
    "\n",
    "But we only have a sample, which of course consists of a very limited number of observations, far from infinity. So we cannot estimate the expectation, only the sample mean, which is written as\n",
    "$$\\hat{\\mu}=\\dfrac{1}{n} \\sum_{i=1}^{n<<\\infty}x_{i}.$$\n",
    "\n",
    "Therefore, we do not use probabilities, but observed frequencies. It's obvious that $\\mathbb{E}[X] \\neq \\hat{\\mu}$, but $\\hat{\\mu} = \\mathbb{E}[X] + \\mathcal{bias}$, where $\\mu$ is the estimate of the expectation, and the bias is some systematic shift between the true and estimated values of the expectation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448ee6e",
   "metadata": {},
   "source": [
    "### General Concept of the Jacknife Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d5912",
   "metadata": {},
   "source": [
    "We have considered a special case. We can now move on to the more general concept of estimator bias. Let's consider the random variable *X* with the unknown distribution *U*. There is a parameter of its distribution called as $\\theta$. And we want to determine its value. Using the abstract parameter $\\theta$ instead of a specific one emphasizes the universality of the Jackknife method, which is able to detect bias for any parameter of the distribution and automatically correct its linear component. We also have the parameter $\\hat{\\theta}$, which is the sampling estimate obtained by using some function. Due to the fact that $\\hat{\\theta}$ was obtained from a sample, while we want to estimate $\\theta$ for the general population, i.e., the entire market in the context of valuation, $\\hat{\\theta}$ has a bias relative to $\\theta$. Mathematically, this means that the expectation for $\\hat{\\theta}$ is not equal to the expectation for $\\theta$:\n",
    "$$\\mathbb{E}(\\hat{\\theta}) \\neq \\mathbb{E}(\\theta).$$\n",
    "In this case, we can say that\n",
    "$$\\mathbb{E}(\\hat{\\theta}_{n}) = \\theta + \\frac{\\alpha}{n} + \\frac{\\beta}{n^{2}} + \\frac{\\gamma}{n^{3}} + \\ldots \\frac{\\omega}{n^{(k\\rightarrow \\infty)}},$$\n",
    "where $\\theta$ is the true value of the parameter for the general population, and  $\\frac{\\alpha}{n} + \\frac{\\beta}{n^{2}} + \\frac{\\gamma}{n^{3}} + \\ldots \\frac{\\omega}{n^{(k\\rightarrow \\infty)}}$ are linear, quadratic, cubic, and other components of the bias. All components decrease as the sample grows according to linear, quadratic, cubic, and other functions. The linear term introduces the largest error because it decreases the slowest of all the other terms.\n",
    "\n",
    "The Jackknife method eliminates the linear component of the bias. Let's introduce some new definitions.\n",
    "\n",
    "$\\hat{\\theta}_{i}$ is the value of $\\hat{\\theta}$ that would be obtained if the calculation were not based on a full sample, but on a sample with an excluded observation *i* that takes values from 1 to *n*. Then\n",
    "$$\\mathbb{E}(\\hat{\\theta}_{(i)}) = \\theta + \\frac{\\alpha}{n-1} + \\frac{\\beta}{(n-1)^{2}} + \\frac{\\gamma}{(n-1)^{3}} + \\ldots \\frac{\\omega}{(n-1)^{(k\\rightarrow \\infty)}}.$$\n",
    "$\\overline{\\theta}$ is the mean value of all $\\hat{\\theta}_{i}$.\n",
    "$$\\overline{\\theta} = \\frac{1}{n} \\sum_{i=1}^{n} \\hat{\\theta}_{i}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af766cda",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb9d14",
   "metadata": {},
   "source": [
    "Therefore, to apply the Jackknife method, i.e., to detect the presence of a bias and automatically eliminate its linear component, the following set of steps is required.\n",
    "1. Suppose we need to estimate some parameter $\\theta$ of a random variable *X*.\n",
    "1. Let's get some an estimate of $\\hat{\\theta}$ for the sample using a mathematical function $\\hat{\\theta}=F(x_{1},\\ldots,x_{n})$.\n",
    "1. $\\hat{\\theta}$ can be biased.\n",
    "1. $\\theta = \\mathbb{E}(\\hat{\\theta}) + bias$.\n",
    "1. Let's create the new *n* samples by sequentially excluding of one *x* from the initial sample.\n",
    "1. Calculate the $\\hat{\\theta}_{(i)}$ for all new samples using the same function **F**.\n",
    "1. Calculate the mean of all $\\hat{\\theta}_{(i)}$ and label it as $\\overline{\\theta}$.\n",
    "1. Calculate the bias using the following formula\n",
    "$$\\widehat{bias}_{jack} = (n-1)(\\overline{\\theta} - \\hat{\\theta}).$$\n",
    "1. Eliminate the linear component of the bias by using the formula\n",
    "$$\\hat{\\theta}_{jacked} = \\hat{\\theta} - \\widehat{bias}_{jack}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce37df90",
   "metadata": {},
   "source": [
    "## Python practical implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe053d2",
   "metadata": {},
   "source": [
    "Today we will use a dataset containing 34821 observations of the residential real estate market in St. Petersburg. It was obtained from web scraping in September 2021. Let's assume that this dataset contains data about the whole market, so we can use it as a general population. Next, we will create a subsample with only 25 observations, which is the typical number of observations that an appraiser deals with. We will calculate the \"expectation\" for our \"general population\", then we will calculate the mean for the sample. Finally, we will apply the Jackknife method and compare the result of the mean calculation to the sample mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c94265b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.stats import jackknife_resampling\n",
    "from astropy.stats import jackknife_stats\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "251899e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0                                     links   price_m  county\n",
      "0               1  https://spb.cian.ru/sale/flat/262765174/  155460.0  sadadm\n",
      "1               2  https://spb.cian.ru/sale/flat/263280601/  295455.0  sadadm\n",
      "2               3  https://spb.cian.ru/sale/flat/261612519/  310559.0  sadadm\n",
      "3               4  https://spb.cian.ru/sale/flat/263094016/  100000.0  sadadm\n",
      "4               5  https://spb.cian.ru/sale/flat/262339898/  145929.0  sadadm\n",
      "...           ...                                       ...       ...     ...\n",
      "34816       34817  https://spb.cian.ru/sale/flat/256621764/   70093.0  llobol\n",
      "34817       34818  https://spb.cian.ru/sale/flat/261430727/   67227.0  llobol\n",
      "34818       34819  https://spb.cian.ru/sale/flat/246538655/   86207.0  llobol\n",
      "34819       34820  https://spb.cian.ru/sale/flat/246587468/   65455.0  llobol\n",
      "34820       34821  https://spb.cian.ru/sale/flat/239698989/   89041.0  llobol\n",
      "\n",
      "[34821 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "df = pd.read_csv(\"spba-flats-210928.csv\", index_col=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0aa0c64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of unit price for 'general population' is 176116.52505671864\n",
      "The maximum of unit price for 'general population' is 1624829.0\n"
     ]
    }
   ],
   "source": [
    "# calculate mean and maximum for the \"general population\"\n",
    "gp_mean = df['price_m'].mean()\n",
    "gp_max = df['price_m'].max()\n",
    "print(\"The mean of unit price for 'general population' is\", gp_mean)\n",
    "print(\"The maximum of unit price for 'general population' is\", gp_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba838cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0                                     links   price_m  county\n",
      "20606       20607  https://spb.cian.ru/sale/flat/264064476/  213889.0  sprkol\n",
      "11160       11161  https://spb.cian.ru/sale/flat/261519367/  161667.0  skryuz\n",
      "21112       21113  https://spb.cian.ru/sale/flat/263895261/  148830.0  sprlax\n",
      "19612       19613  https://spb.cian.ru/sale/flat/250041085/  189706.0  sprn65\n",
      "5076         5077  https://spb.cian.ru/sale/flat/263660571/  139382.0  skapis\n",
      "13890       13891  https://spb.cian.ru/sale/flat/263013521/  231935.0  smonow\n",
      "12315       12316  https://spb.cian.ru/sale/flat/262914352/  198765.0  smozwy\n",
      "8250         8251  https://spb.cian.ru/sale/flat/263518941/  181985.0  skrpol\n",
      "243           244  https://spb.cian.ru/sale/flat/251281327/  210697.0  sadeka\n",
      "15608       15609  https://spb.cian.ru/sale/flat/260189091/  134454.0  snenew\n",
      "7387         7388  https://spb.cian.ru/sale/flat/257597416/  108974.0  skomet\n",
      "5179         5180  https://spb.cian.ru/sale/flat/261053782/  150000.0  skapis\n",
      "22767       22768  https://spb.cian.ru/sale/flat/261935726/  121429.0  spryun\n",
      "22444       22445  https://spb.cian.ru/sale/flat/262417793/  223214.0  spryun\n",
      "10338       10339  https://spb.cian.ru/sale/flat/259129106/  177384.0  skryug\n",
      "13155       13156  https://spb.cian.ru/sale/flat/262525257/  195000.0  smozwy\n",
      "804           805  https://spb.cian.ru/sale/flat/259194918/  138900.0  sadkol\n",
      "18970       18971  https://spb.cian.ru/sale/flat/262743501/  221212.0  sprn65\n",
      "21243       21244  https://spb.cian.ru/sale/flat/261041315/  233333.0  sproze\n",
      "28099       28100  https://spb.cian.ru/sale/flat/261144927/  110754.0  spushu\n",
      "31551       31552  https://spb.cian.ru/sale/flat/262688402/   83459.0  lwswse\n",
      "4399         4400  https://spb.cian.ru/sale/flat/260387424/  151282.0  skan21\n",
      "12014       12015  https://spb.cian.ru/sale/flat/257553955/  198780.0  smogag\n",
      "22696       22697  https://spb.cian.ru/sale/flat/264049058/  145516.0  spryun\n",
      "23239       23240  https://spb.cian.ru/sale/flat/261418052/  204433.0  swagaw\n"
     ]
    }
   ],
   "source": [
    "# create sample\n",
    "sam_size = 25\n",
    "ran_sam = df.sample(n=sam_size)\n",
    "print(ran_sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "edc66695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of unit price for random sample is 147030.5\n",
      "The maximum of unit price for random sample is 181481.0\n"
     ]
    }
   ],
   "source": [
    "# calculate mean and maximum for the samle\n",
    "rs_mean = ran_sam['price_m'].mean()\n",
    "rs_max = ran_sam['price_m'].max()\n",
    "print(\"The mean of unit price for random sample is\", rs_mean)\n",
    "print(\"The maximum of unit price for random sample is\", rs_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e1c480dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20606    213889.0\n",
      "11160    161667.0\n",
      "21112    148830.0\n",
      "19612    189706.0\n",
      "5076     139382.0\n",
      "13890    231935.0\n",
      "12315    198765.0\n",
      "8250     181985.0\n",
      "243      210697.0\n",
      "15608    134454.0\n",
      "7387     108974.0\n",
      "5179     150000.0\n",
      "22767    121429.0\n",
      "22444    223214.0\n",
      "10338    177384.0\n",
      "13155    195000.0\n",
      "804      138900.0\n",
      "18970    221212.0\n",
      "21243    233333.0\n",
      "28099    110754.0\n",
      "31551     83459.0\n",
      "4399     151282.0\n",
      "12014    198780.0\n",
      "22696    145516.0\n",
      "23239    204433.0\n",
      "Name: price_m, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# obtain Jackknife resamples\n",
    "new_df = ran_sam[\"price_m\"]\n",
    "array = new_df.to_numpy()\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "da61eb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697. 134454.\n",
      "  108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 148830. 189706. 139382. 231935. 198765. 181985. 210697. 134454.\n",
      "  108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 189706. 139382. 231935. 198765. 181985. 210697. 134454.\n",
      "  108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 139382. 231935. 198765. 181985. 210697. 134454.\n",
      "  108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 231935. 198765. 181985. 210697. 134454.\n",
      "  108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 198765. 181985. 210697. 134454.\n",
      "  108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 181985. 210697. 134454.\n",
      "  108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 210697. 134454.\n",
      "  108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 134454.\n",
      "  108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  134454. 150000. 121429. 223214. 177384. 195000. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  134454. 108974. 121429. 223214. 177384. 195000. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  134454. 108974. 150000. 223214. 177384. 195000. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  134454. 108974. 150000. 121429. 177384. 195000. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  134454. 108974. 150000. 121429. 223214. 195000. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  134454. 108974. 150000. 121429. 223214. 177384. 138900. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  134454. 108974. 150000. 121429. 223214. 177384. 195000. 221212. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  134454. 108974. 150000. 121429. 223214. 177384. 195000. 138900. 233333.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  134454. 108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212.\n",
      "  110754.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  134454. 108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212.\n",
      "  233333.  83459. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  134454. 108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212.\n",
      "  233333. 110754. 151282. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  134454. 108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212.\n",
      "  233333. 110754.  83459. 198780. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  134454. 108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212.\n",
      "  233333. 110754.  83459. 151282. 145516. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  134454. 108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212.\n",
      "  233333. 110754.  83459. 151282. 198780. 204433.]\n",
      " [213889. 161667. 148830. 189706. 139382. 231935. 198765. 181985. 210697.\n",
      "  134454. 108974. 150000. 121429. 223214. 177384. 195000. 138900. 221212.\n",
      "  233333. 110754.  83459. 151282. 198780. 145516.]]\n"
     ]
    }
   ],
   "source": [
    "# obtain Jackknife resamples\n",
    "resamples = jackknife_resampling(array)\n",
    "print(resamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "49c4bee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 24)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain Jackknife resamples shape\n",
    "resamples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2210b795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the jacked mean is  170999.2\n",
      "the bias got from the Jackknife is  23968.70000000001\n",
      "the standard error got from the Jackknife is  8468.584368318783\n",
      "the confidence interval (95%) of jacked mean is  [154401.07963806 187597.32036194]\n"
     ]
    }
   ],
   "source": [
    "# obtain Jackknife estimate for the mean, its bias,\n",
    "# its standard error, and its 95% confidence interval\n",
    "test_statistic = np.mean\n",
    "\n",
    "estimate, bias, stderr, conf_interval = jackknife_stats(\n",
    "    array, test_statistic, 0.95)\n",
    "\n",
    "mean_jacked = estimate\n",
    "print(\"the jacked mean is \", mean_jacked)\n",
    "bias_jack = abs(mean_jacked - rs_mean)\n",
    "print(\"the bias got from the Jackknife is \", bias_jack)\n",
    "std_error = stderr\n",
    "print(\"the standard error got from the Jackknife is \", std_error)\n",
    "conf_int = conf_interval\n",
    "print(\"the confidence interval (95%) of jacked mean is \", conf_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75b9a6",
   "metadata": {},
   "source": [
    "As we can see, the expectation is 176116.525, the sample mean is 147030.500, and the adjusted mean obtained by the jackknife method is 170999.200. The confidence interval for the expectation is [154401.080, 187597.320] with a probability of 0.95. Thus, we have clearly proved that the application of the Jackknife method leads to a significant improvement in the accuracy of the distribution parameter estimation. Moreover, the calculated confidence interval contains the true value of the expectation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c12c8eb",
   "metadata": {},
   "source": [
    "## Afterword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11333311",
   "metadata": {},
   "source": [
    "The Jackknife method is a simple and computationally efficient tool for adjusting sampling estimators. I hope that its application will help many appraisers in their daily practice. And perhaps it will inspire someone to use machine learning methods more widely in valuation activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc473e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
