\documentclass[]{scrartcl}
\input{standard_preamble.tex}
%opening
\title{Информационный критерий Акаике (\foreignlanguage{english}{Akaike's information criterion, AIC})}
\author{К.\,А.\,Мурашев}

\begin{document}

\maketitle

\begin{abstract}
Современный оценщик в~своей практике часто сталкивается с~необходимостью выбора конкретной регрессионной модели с~точки зрения включения либо невключения в~неё отдельных предикторов (ценообразующих факторов). В~данном фрагменте рассматривается Информационный критерий Акаике, предназначенный для~осуществления выбора такой регрессионной модели, которая позволяет в~достаточной мере описать данные, используя при~этом минимальное число предикторов, что~отчасти позволяет устранить проблему переобучения модели.
\end{abstract}

\section{Общие сведения}
Критерий Акаике представляет собой \href{https://ru.wikipedia.org/wiki/Информационный_критерий}{информационный критерий}~\cite{Wiki:IC} выбора наилучшей модели из~нескольких параметризованных регрессионных моделей, имеющих разное число предикторов. Критерий основан на~понятии расстояния \href{https://ru.wikipedia.org/wiki/Расстояние_Кульбака_—_Лейблера}{Кульбака--Лейбнера}~\cite{Wiki:Kullback-Leibler}, являющегося мерой удалённости двух вероятностных распределений относительно друг друга и~способного помочь определить расстояние между двумя моделями. Применение данного критерия основывается на~\href{http://www.machinelearning.ru/wiki/index.php?title=Бритва_Оккама}{Принципе Оккама}~\cite{MLRU:Okkama}, согласно которому, применительно к~регрессионному анализу, можно сказать, что~лучшей моделью является та, которая в~достаточной мере полно описывает данные, используя при~этом наименьшее число предикторов. Данный критерий был разработан в~начале 1970-х годов японским исследователем \href{https://ru.wikipedia.org/wiki/Акаикэ,_Хироцугу}{Хироцугу Акаике}~\cite{Wiki:Akaike-Xiroczugu}.

\section{Описание критерия}
Расстояние Кульбака--Лейблера между двумя непрерывными функциями представляет собой интеграл.

\begin{equation}\label{Kullback-Leibler-Dist}
I(f, g) = \int{f(x)ln}\frac{f(x)}{g(x|\theta)} \times d(x)
\end{equation}
Оценка расстояния между двумя моделями при~этом может быть осуществлена на~основе величины:
\begin{equation}\label{AIC-dist-between-two-models}
E_{\hat{\theta}}[I(f, \hat{g})],
\end{equation}
где $ \hat{\theta} $ "--- оценка вектора параметров, в~состав которого входят параметры модели и~случайные величины,

$ \hat{g} = g(\cdot|\hat{\theta}) $.

При~этом максимум логарифмической функции правдоподобия и~оценка матожидания связаны следующим выражением:
\begin{equation}\label{AIC-likehood}
\log(L(\hat{\theta}|y)) - K = Const - \hat{E}_{\hat{\theta}}[I[f, \hat{g}]],
\end{equation}
где $ K $ "--- число параметров модели,

L "--- максимум логарифмической \href{https://ru.wikipedia.org/wiki/Функция_правдоподобия}{функции правдоподобия}~\cite{Wiki:likelihood-function, MLRU:likelyhood-function}.

Таким образом вместо вычисления расстояния между моделями можно ввести оценивающий критерий.
\begin{equation}\label{AIC-criterion}
AIC = 2K -2log(L(\hat{\theta}|y))
\end{equation}
В~интересующем нас случае применения критерия с~целью выбора наилучшей регрессионной модели можно использовать следующую формулу данного критерия, основанную на~сумме квадратов остатков (SSE):
\begin{equation}\label{AIC-regression}
AIC = 2K + n[ln(\delta^2)],
\end{equation}
где
\begin{equation}\label{AIC-SSE}
SSE = \lvert f(x_i) - y_i\rvert = \sum_(i=1)^(n)(y_i - f(\omega, x_i))^2
\end{equation}
\begin{equation}\label{AIC-SSE-2}
\delta^2 = \frac{SSE}{N-2}
\end{equation}
В~случае использования моделей с~различным количеством наблюдений (объектов-аналогов) выражение принимает вид.
\begin{equation}\label{AIC-diff-N}
AIC=2K + n[\ln(\frac{2\pi \mathit{RSS}}{n}) + 1]
\end{equation}
Наилучшей является та~модель, значение AIC которой минимально. При~этом само значение AIC не~является содержательным и~служит только для~сравнения моделей.

\section{Особенности применения критерия}
\begin{itemize}
	\item Критерий не~только вознаграждает за~качество приближения, но~и~штрафует за~использование излишнего количества предикторов.
	\item Штраф за~число предикторов ограничивает значительный рост сложности модели. 
	\item Порядок выбора моделей не имеет значения.
\end{itemize}

\section{Модификации критерия}
$\mathbf{AIC_{c}}$ используется в~случае работы с~относительно небольшим числом наблюдений, когда $\frac{n}{K} \leq 40$. В~то~же время при~наличии значительного числа наблюдений $\frac{n}{K} > 40$ возможно применение обоих вариантов, хотя чаще рекомендуется использование базового варианта AIC.  Особенность критерия $AIC_{c}$ заключается в~том, что~функция штрафа умножается на~поправочный коэффициент:
\begin{equation}\label{AICc-1}
AIC_{c} = AIC + \frac{2K(K+1)}{n-K-1},
\end{equation}
При~этом данное выражение эквивалентно:
\begin{equation}\label{AICc-2}
AIC_{c} = \ln\frac{SSE}{n} + \frac{n+K}{n-K-2}
\end{equation} 
\textbf{QAIC} следует использовать для~моделей, в~которых часть предикторов являются случайными величинами с~простыми дискретными распределениями (биномиальное, пуассоновское и~т.\,д.). В~таких случаях используется более общая модель, которая получается из~рассматриваемой добавлением параметра обобщённого распределения. Оценка параметра определяется как~распределение $\chi^2$. В~таком случае значение параметра как~правило находится на~отрезке $ c \in[1:4]$. В~случае, когда $\hat{c} \leq 1$ следует выполнить замену на~ $\hat{c} = 1$. При~$\hat{c} = 1$ QAIC сводится к AIC.
\begin{equation}\label{QAIC}
QAIC = 2K - \frac{ln(L)}{\hat{c}}
\end{equation}
\begin{equation}\label{QAICc}
QAIC_{c} = QAIC + \frac{2K(K+1)}{n-K-1}
\end{equation}

\section{Практическая реализация}
В~языках R и Python существуют функции, позволяющие осуществлять автоматический отбор моделей на~основе AIC и~его~модификаций. В~частности, в~языке R существует библиотека <<MASS>>, содержащая функцию \textbf{stepAIC}, позволяющую автоматически отобрать регрессионную модель, являющуюся наилучшей с~точки зрения AIC. В~языке Python нет~отдельной функции, позволяющей оптимизировать модель по~критерию AIC, однако данный критерий можно указать в~качестве аргумента при~построении лассо-регрессии. Также возможно написание собственного кода, выполняющего пошаговую оптимизацию модели. Для~оценщика, понимающего суть метода и~формулы, приведённые выше, также не~составит труда написать алгоритм расчёта AIC в~табличном процессоре. Однако, последнее решение не~соответствует лучшим практикам и~потому не~будет рассматриваться в~данной работе.

Рассмотрим пример. Предположим, что~существует зависимая переменная y, а~также 8 предикторов, записанных в~переменные v1, v2 $\ldots$ v7, v8, записанные в~единый датафрейм \textbf{df}. Задача состоит в~том, чтобы построить модель, включающую в~себя минимальный необходимый и~достаточный набор предикторов. В~качестве допущения укажем, что~переменные являются независимыми. Для~построения модели, оптимизированной методом AIC, достаточно написать простой код, примеры которого приводятся в~листингах~\ref{listing-AIC-R-1}, \ref{listing-AIC-Python-1}. 

\begin{lstlisting}[float, caption = Реализация на~языке R, firstnumber=1, language = R, label= listing-AIC-R-1]
	model_aic <- stepAIC(lm(y ~ (.), data = df))
	summary(model)
\end{lstlisting}

\begin{lstlisting}[float, caption = Реализация на~языке Python, firstnumber=1, language = Python, label = listing-AIC-Python-1]
AICs = {}
for k in range(1,len(predictorcols)+1):
for variables in itertools.combinations(predictorcols, k):
predictors = train[list(variables)]
predictors['Intercept'] = 1
res = sm.OLS(target, predictors).fit()
AICs[variables] = 2*(k+1) - 2*res.llf
pd.Series(AICs).idxmin()
\end{lstlisting}

\section{Выводы}
Проверка качества регрессионной модели и~её~оптимизация является важной частью процесса оценки. К~сожалению, некоторые оценщики используют лишь один критерий "--- коэффициент детерминации~($R^2$), забывая о~необходимости проверки p-значений как~всей модели, так~и~каждого предиктора, а~также необходимости её~оптимизации, одним из~методов которой является AIC. 
\nocite{Wiki:AIC, MLRU:AIC}
\printbibliography[title=Источники информации]

\end{document}
